# 构建爬虫群很简单，掌握 python 基础功就能做到


### 关键词：

爬虫，编程，python，批量文件操作

### 描述：

目前准备爬取一家目标网站的公开数据，预测将累计发起 百万次 api 请求，每个爬虫脚本都是单进程、单线程。把想要爬取的数据，按关键字段（如id）拆分多段（每段为 10000 条数据），分别安排给不同的爬虫同时爬取。这些爬虫，我称之为爬虫群。爬取到的数据我都存储在本地的mysql中。

我是如何实现爬虫群呢？用的是 jupyter lab，创建N个 .ipynb 文件，每个文件创建一个 cell，并把初始爬虫脚本拷贝进去，并修改关键参数（如起始id），然后启动该cell，即激活该爬虫。

如果主动关闭爬虫群，或因为异常重启爬虫群，关键参数（如起始id），需要检测已爬取的情况，修改该值然后重新启动。

创建 20 多个爬虫文件，修改每个爬虫文件中的 id ，用手工操作太低效。这是一个可以优化的地方。

### 疑问：

如何用 python 脚本实现自动化检测已爬取的情况，修改关键参数，并批量生成或修改爬虫群呢？

### 思路：

1、需求1：检测已爬取的情况。所需知识技能：python 读取 mysql数据，并对数据进行统计分析。我用到 pymysql 和 pandas。

2、需求2：修改关键参数，批量生成爬虫群（N个文件）。所需知识技能：python 批量创建文件。很简单，for循环+写文件。

### 结论：

共用2段脚本分别完成需求 1 和 2。实际使用时，先执行脚本1，再执行脚本2（当然也可以把两个脚本放在一块，一次性执行完毕）。


1、需求1的脚本（关键信息已模糊处理）

```python

import datetime
import pandas as pd
import pymysql

connect = pymysql.connect('localhost','username','password','database_name')
cursor = connect.cursor()

sql_search = 'SELECT `id` FROM `table_name`;'
users_data = pd.read_sql(sql_search,connect)

connect.close()
cursor.close()

print(datetime.datetime.now(),'准备就绪')

ids = users_data['id'].to_frame()
id_lvs = []
for id_lv in range(0,1200000,10000):
    try:
        x =ids[(ids['id']>=id_lv) & (ids['id']<(id_lv+10000))]
        y = x['id'].values.max()
        id_lvs.append((id_lv,y))
    except ValueError:
        continue
else:
    rlt = pd.DataFrame(id_lvs,columns=["from_id","got_to"])

```

2、需求2的脚本（关键信息已模糊处理）


```python

import os.path
import sys

def check_mkdir(path):
    path=path.strip()
    path=path.rstrip("\\")
    isExists=os.path.exists(path)
    if not isExists:
        os.makedirs(path)
        print(datetime.datetime.now(), ' 创建文件夹 ', path)
    else:
        print(datetime.datetime.now(), ' 文件夹已存在 ', path)

path = "dir_name/"
check_mkdir(path)


file_content_1 = r"""{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬虫脚本片段1","""


file_content_2 = r"""\n",
    "#爬虫脚本片段2"""



file_content_3 = r"""\n",
    "#爬虫脚本片段3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

"""

for got_to in rlt['got_to'].values:
    start_idi = (got_to//10000)*10000
    file_content = file_content_1 + str(got_to+1) + file_content_2 + str(start_idi+10000) + file_content_3
    f = open('./dir_name/dir_name%s'%start_idi + '.ipynb',"a",encoding='utf-8')
    f.write(file_content)
    f.close()

print(datetime.datetime.now(),"批量创建完毕") 

```

请留意，在脚本2中，我把爬虫脚本的具体内容已经模糊掉了。
